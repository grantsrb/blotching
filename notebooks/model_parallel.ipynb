{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3913db94-e409-407e-a6e7-8a28b0ff6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer, GPTJForCausalLM, DataCollatorWithPadding\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07766e72-08e3-400c-aed3-25c9b69365c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_type = \"hf-internal-testing/tiny-random-gptj\"\n",
    "model_type = \"gpt2-medium\"\n",
    "tokenizer =   AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "hface_model = AutoModelForCausalLM.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0573a3-8595-4f82-9995-92713bcf91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284f1575-6b0f-4e65-a5f6-775c93e24def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfa8e1b-3670-45f7-ad12-149e4ceae4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hface_model.deparallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b193150-060c-4522-af88-9de5f1ac3088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore this for now\n",
    "#small_dataset = {\n",
    "#    \"rand\": {\n",
    "#        \"prompts\": [\n",
    "#            \"he's bald, he's mauled, he's called grindlewald... or so I'm tald.\",\n",
    "#            \"To win, you must move forward at all costs. There is no retreat. I get knocked down, but I get up again. You will never keep me down.\",\n",
    "#            \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam\", \n",
    "#            \"quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum\",\n",
    "#            \"dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"But why does the pilgrim seek refuge?\",\n",
    "#            \"Backoff, headstrong\",\n",
    "#            \"Sed ut perspiciatis unde omnis iste natus error\",\n",
    "#            \"sit voluptatem accusantium doloremque laudantium\",\n",
    "#            \"totam rem aperiam, eaque ipsa quae ab illo inventore\",\n",
    "#        ]\n",
    "#    },\n",
    "#    \"blicket_direct\": {\n",
    "#        \"prompts\": [\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine does not turn on. Is the item a blicket?\",\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the item a blicket?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fail to fix it. Is the tool you used magic?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool you used magic?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins wobbly. Is that egg hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Is that egg hardboiled?\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#            \"no, it is not\",\n",
    "#            \"yes, it is\",\n",
    "#        ]\n",
    "#    },\n",
    "#    \"blicket_implicit\": {\n",
    "#        \"prompts\": [\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and the machine turns on, the item is a blicket. You take one of the items and place it on a machine. The machine does not turn on. Is the other item a blicket?\",\n",
    "#            \"You have two items. At least one is a blicket. If an item touches a machine and the machine turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the other item a blicket?\",\n",
    "#            \"You have two items. Only one is a blicket. If an item touches a machine and it turns on, the item is a blicket. You take one of the items and place it on a machine. The machine turns on. Is the other item a blicket?\",\n",
    "#            \n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fail to fix it. Is the tool you did not use magic?\",\n",
    "#            \"You have two tools. At least one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool that you did not use magic?\",\n",
    "#            \"You have two tools. Only one of the tools is magic. If a tool is magic, it can fix anything. If a tool is not magic, it cannot fix anything. You take one of the tools and try to fix a washing machine. You fix it! Is the tool that you did not use magic?\",\n",
    "#            \n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins wobbly. Are the other eggs all hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at least 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Are the other eggs all hardboiled?\",\n",
    "#            \"I see four eggs sitting on a table. The waiter told me that at only 3 of the eggs are hard boiled. If an egg is hard boiled, it will spin evenly. If it is not, it will spin wobbly. I see the waiter take one of the eggs and spin it. The egg spins evenly. Are the other eggs all hardboiled?\",\n",
    "#        ],\n",
    "#        \"answers\": [\n",
    "#            \"yes, it is\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, it is not\",\n",
    "#            \n",
    "#            \"yes, it is\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, it is not\",\n",
    "#            \n",
    "#            \"yes, they are\",\n",
    "#            \"we cannot know the answer from the given information\",\n",
    "#            \"no, they are not\",\n",
    "#        ]\n",
    "#    }\n",
    "#}\n",
    "#\n",
    "#string = small_dataset[\"rand\"][\"prompts\"][0]\n",
    "#string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3e3dd6-73c3-47eb-b76e-0190f413a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "#\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931728e2-f3d0-47f2-8496-9634d6b02ccb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8439d3b5-8b17-4d5b-9816-0eae65960a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "bsize = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a4098d-f972-4421-8217-97a99104e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/grantsrb/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2146e353-879f-4ed0-8a04-0d0288b22f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2-medium', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab4aa8e-4d48-45e6-bbc3-d591130f9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = {\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"cls_token\": \"<CLS>\", # Using CLS token as compression token\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da86b81-3142-487a-8f66-7799224e3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added = tokenizer.add_special_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764dcf59-d118-4d8f-add8-914f6b06ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,h = hface_model.transformer.wte.weight.shape\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9376f41-9248-433c-bc0a-ddacdbc9ec9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hface_model.resize_token_embeddings(n+num_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a5c044-0bfa-49b0-8874-f61fa248c6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heyo bo<CLS>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"heyo bo\"\n",
    "x = s[:100] + new_tokens[\"cls_token\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d572ece-a2f9-4e3b-aa6f-f01b0e1c1af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/grantsrb/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3c66e6bbed442bd8.arrow\n"
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    sent1 = [s + tokenizer.cls_token for s in examples[\"sentence1\"]]\n",
    "    inpts = tokenizer(sent1,padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True, return_tensors=\"pt\")\n",
    "    sent2 = [s + tokenizer.eos_token for s in examples[\"sentence2\"]]\n",
    "    outs = tokenizer(sent2,padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True , return_tensors=\"pt\")\n",
    "    \n",
    "    examples[\"label\"] = torch.LongTensor(examples[\"label\"])\n",
    "    idx = examples[\"label\"]==0\n",
    "    \n",
    "    outs[\"input_ids\"][idx] = inpts[\"input_ids\"][idx].clone()\n",
    "    outs[\"attention_mask\"][idx] = inpts[\"attention_mask\"][idx].clone()\n",
    "    outs[\"input_ids\"][outs[\"input_ids\"]==tokenizer.cls_token_id] = tokenizer.eos_token_id\n",
    "    \n",
    "    idx = (inpts[\"input_ids\"]==tokenizer.cls_token_id).float().reshape(len(idx),-1).sum(-1)\n",
    "    if idx.sum() != len(idx):\n",
    "        i = torch.argmax((idx==0).float())\n",
    "        inpts[\"input_ids\"][i,-1] = tokenizer.cls_token_id\n",
    "    idx = (outs[\"input_ids\"]==tokenizer.eos_token_id).float().reshape(len(examples[\"label\"]),-1).sum(-1)\n",
    "    if idx.sum() != len(examples[\"label\"]):\n",
    "        i = torch.argmax((idx==0).float())\n",
    "        outs[\"input_ids\"][i,-1] = tokenizer.eos_token_id\n",
    "    \n",
    "    #s = (inpts[\"input_ids\"]==tokenizer.cls_token_id).float().reshape(len(idx),-1).sum()\n",
    "    #if s != len(inpts[\"input_ids\"]):\n",
    "    #    print(\"CLS token id:\", tokenizer.cls_token_id)\n",
    "    #    print(\"Sum:\", s)\n",
    "    #    print(\"Len:\", len(inpts[\"input_ids\"]))\n",
    "    #    x = inpts[\"input_ids\"]\n",
    "    #    idx = (x==tokenizer.cls_token_id).float().reshape(x.shape[0],-1).sum(-1)\n",
    "    #    idx = torch.argmax((idx==0).float())\n",
    "    #    print(\"idx:\", idx)\n",
    "    #    print(sent1[idx])\n",
    "    #    print(inpts[\"input_ids\"][idx])\n",
    "    #    print()\n",
    "    #    print()\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\":        inpts[\"input_ids\"],\n",
    "        \"attention_mask\":   inpts[\"attention_mask\"],\n",
    "        \"output_ids\":       outs[\"input_ids\"],\n",
    "        \"output_attn_mask\": outs[\"attention_mask\"],\n",
    "        \"labels\":           examples[\"label\"],\n",
    "    }\n",
    "    \n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e34b702-5c35-420b-9100-44ee533460c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask', 'output_ids', 'output_attn_mask', 'labels'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d48426f-b51e-4413-904d-58a670d43a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf5b4cbf-a41d-4877-8639-7e6580960888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"output_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea7816f6-8d0f-49dd-af78-8a8821ae3f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'output_ids', 'output_attn_mask', 'labels'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\", \"label\"])\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f2d5d9a-2444-461b-bcdb-e7cb24ee310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type=\"torch\")\n",
    "#dataset.set_format(type=\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6ea042-9f30-4528-a4af-aa557f27bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  464,  1705,  1908, 28289,  7303,   319,   257, 11626,   287,   706,\n",
      "           12, 24425,  7313,   764, 50258, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])\n",
      "tensor([  464,  1705,  1908, 28289,  7303,   319,   257, 11626,   287,   706,\n",
      "           12, 24425,  7313,   764, 50256, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dataloader))\n",
    "idx = data[\"labels\"]==0\n",
    "print(data[\"input_ids\"][idx][0])\n",
    "print(data[\"output_ids\"][idx][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a8a0c0-6a1e-42f1-972a-0c9666f2bb9b",
   "metadata": {},
   "source": [
    "## Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a3380d4-361d-4ce6-9ca4-8d8149b902f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hface_model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d27c250-9e08-407c-8245-4c83041751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "device = next(hface_model.parameters()).get_device()\n",
    "with torch.no_grad():\n",
    "    inpts = {\n",
    "        \"input_ids\": data[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "    }\n",
    "    outputs = hface_model.transformer(**inpts)\n",
    "    #logits = outputs.logits\n",
    "    #print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef602f2d-de5c-42d5-8c7b-8973a91c5291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0b1516-2ddf-4c1f-80ba-bf9f4bca60fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"last_hidden_state\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02cffb0-4add-4d08-a792-c654936cef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[\"past_key_values\"]) # Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a065f25-fa43-47ff-8a81-a596b1a8bcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 100, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"past_key_values\"][0][0].shape # Keys of first layer (or values, I'm not sure which, but probably keys because of naming key_values order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b858f69-3bec-46f8-ad93-6633a625dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transformer(inputs_embeds=outputs[\"last_hidden_state\"], attention_mask=inpts[\"attention_mask\"]) # Use this to argue embeddings instead of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c13ca2-32df-4d4e-be45-117fc3e20a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae91e0c-29c6-4c3e-91ef-0e42ffff2e39",
   "metadata": {},
   "source": [
    "Positive Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a41a259-9652-44ea-aa27-00b3d459f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nursing schools turned away more than 5,000 qualified applicants in the past year because of shortages of faculty and classroom space.<CLS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09f7e54b-0b94-4428-bdcc-4267924bbe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nursing schools turned away more than 5,000 qualified applicants in the past year because of shortages of faculty and classroom space.<|endoftext|><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"output_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8f1c2-125e-40e5-a28e-268a3d9dfdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533641e8-2780-4a4c-92a1-2220faf95bcc",
   "metadata": {},
   "source": [
    "\n",
    "Negative Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fffdd09e-33db-4f18-a003-ce80d68968d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Java Enterprise System bundles a slew of Sun software for a yearly subscription of $ 100 per employee.<CLS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b5fad74-7838-4034-b246-3fc370e7d81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Java Enterprise System bundles a slew of Sun software for a yearly subscription of $ 100 per employee.<|endoftext|><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data[\"output_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432791c-40a4-482a-941c-0f7f41895c0a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fabc6aa6-abfd-4697-bd47-63b12585ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAutoEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Trains a new token type to compress a sentence into a single vector representation\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        model: hugging face transformer model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hface_model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.CLS_ID = tokenizer.cls_token_id\n",
    "        self.CLS = tokenizer.cls_token\n",
    "        self.EOS_ID = tokenizer.eos_token_id\n",
    "        self.EOS = tokenizer.eos_token\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        data: dict\n",
    "            \"input_ids\": LongTensor (B,S1)\n",
    "                the token indices of the input sequence. The CLS token should be appended to the end of each sentence.\n",
    "            \"attention_mask\": LongTensor (B,S1)\n",
    "                attention mask for padding purposes. 0s mean padding.\n",
    "            \"output_ids\": LongTensor (B,S2)\n",
    "                the token indices of the target sequence. An EOS token should be appended to the end of each sentence\n",
    "            \"output_attn_mask\": LongTensor (B,S2)\n",
    "                attention mask for padding purposes. 0s mean padding.\n",
    "        \"\"\"\n",
    "        \n",
    "        model = self.hface_model\n",
    "        inpt_embs = model.transformer.wte(data[\"input_ids\"]).data\n",
    "        idx = data[\"input_ids\"]==self.CLS_ID\n",
    "        inpt_embs[idx] = 0\n",
    "        inpt_embs[idx] += model.transformer.wte.weight[self.CLS_ID]\n",
    "        out_embs =  model.transformer.wte(data[\"output_ids\"]).data\n",
    "        \n",
    "        \n",
    "        fx = model.transformer(inputs_embeds=inpt_embs, attention_mask=data[\"attention_mask\"])\n",
    "        fx = fx[\"last_hidden_state\"][idx][:,None]\n",
    "        \n",
    "        \n",
    "        # Concat compressed representation to beginning of sentence\n",
    "        attn = torch.cat([torch.ones_like(data[\"output_attn_mask\"][:,:1]), data[\"output_attn_mask\"]], dim=1)\n",
    "        #attn = torch.pad(data[\"output_attn_mask\"], (1,0))\n",
    "        try:\n",
    "            out_embs = torch.cat([fx,out_embs], dim=1)\n",
    "            attn = torch.cat([torch.ones_like(data[\"output_attn_mask\"][:,:1]), data[\"output_attn_mask\"]], dim=1)\n",
    "        except:\n",
    "            print(\"Data\")\n",
    "            for k in data: print(k, data[k].shape)\n",
    "            print(\"idx sum:\", idx.float().sum())\n",
    "            print(\"In Embds\", inpt_embs.shape)\n",
    "            print(\"Out Embds\", out_embs.shape)\n",
    "            print(\"FX\", fx.shape)\n",
    "            assert False\n",
    "        \n",
    "        preds = model(inputs_embeds=out_embs, attention_mask=attn).logits\n",
    "        return preds\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a1c8d9-fb75-4543-8f65-9319b9424808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceAutoEncoder(hface_model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c9959-6a37-421c-ad6e-cc51248c58e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17624719-7c0e-4909-8675-0f50d0a96c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "l2 = 1e-3\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "483530ff-8e15-4f87-ae88-bbd14d95b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.hface_model.transformer.wte.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2)\n",
    "loss_fxn = torch.nn.CrossEntropyLoss()\n",
    "params = set(params)\n",
    "for p in model.parameters():\n",
    "    if p not in params: p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec7221-89f4-458c-975b-f718d5e1c8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning Epoch 0\n",
      "Loss 84.24694 -- Acc: 0.0\n",
      "Loss 81.21325 -- Acc: 0.0\n",
      "Loss 70.53687 -- Acc: 0.0\n",
      "Loss 75.70064 -- Acc: 0.0\n",
      "Loss 89.97944 -- Acc: 0.0\n",
      "Loss 63.91817 -- Acc: 0.0\n",
      "Loss 67.60391 -- Acc: 0.0\n",
      "Loss 79.08193 -- Acc: 0.0\n",
      "Loss 75.36988 -- Acc: 0.0\n",
      "Loss 87.33172 -- Acc: 0.0\n",
      "Loss 78.30166 -- Acc: 0.0\n",
      "Loss 68.78848 -- Acc: 0.0\n",
      "Loss 78.52134 -- Acc: 0.0\n",
      "Loss 69.13879 -- Acc: 0.0\n",
      "Loss 77.55251 -- Acc: 0.0\n",
      "Loss 69.66534 -- Acc: 0.0\n",
      "Loss 84.58016 -- Acc: 0.0\n",
      "Loss 76.60291 -- Acc: 0.0\n",
      "Loss 87.75251 -- Acc: 0.0\n",
      "Loss 82.21988 -- Acc: 0.0\n",
      "Loss 67.91528 -- Acc: 0.0\n",
      "Loss 83.62623 -- Acc: 0.0\n",
      "Loss 93.74638 -- Acc: 0.0\n",
      "Avg Loss: 17799.98936 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 1\n",
      "Loss 79.82491 -- Acc: 0.0\n",
      "Loss 100.64024 -- Acc: 0.0\n",
      "Loss 70.68429 -- Acc: 0.0\n",
      "Loss 76.25298 -- Acc: 0.0\n",
      "Loss 76.36373 -- Acc: 0.0\n",
      "Loss 75.4203 -- Acc: 0.0\n",
      "Loss 70.43633 -- Acc: 0.0\n",
      "Loss 66.94095 -- Acc: 0.0\n",
      "Loss 60.8101 -- Acc: 0.0\n",
      "Loss 79.07037 -- Acc: 0.0\n",
      "Loss 80.07519 -- Acc: 0.0\n",
      "Loss 74.47204 -- Acc: 0.0\n",
      "Loss 64.73815 -- Acc: 0.0\n",
      "Loss 79.96474 -- Acc: 0.0\n",
      "Loss 75.52843 -- Acc: 0.0\n",
      "Loss 70.97234 -- Acc: 0.0\n",
      "Loss 75.6472 -- Acc: 0.0\n",
      "Loss 87.69759 -- Acc: 0.0\n",
      "Loss 82.31384 -- Acc: 0.0\n",
      "Loss 76.89906 -- Acc: 0.0\n",
      "Loss 77.68832 -- Acc: 0.0\n",
      "Loss 81.7454 -- Acc: 0.0\n",
      "Loss 80.3371 -- Acc: 0.0\n",
      "Avg Loss: 17919.70769 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 2\n",
      "Loss 79.46243 -- Acc: 0.0\n",
      "Loss 74.74407 -- Acc: 0.0\n",
      "Loss 73.89275 -- Acc: 0.0\n",
      "Loss 70.63107 -- Acc: 0.0\n",
      "Loss 83.493 -- Acc: 0.0\n",
      "Loss 80.90493 -- Acc: 0.0\n",
      "Loss 82.6437 -- Acc: 0.0\n",
      "Loss 76.53478 -- Acc: 0.0\n",
      "Loss 80.83591 -- Acc: 0.0\n",
      "Loss 82.3447 -- Acc: 0.0\n",
      "Loss 83.52557 -- Acc: 0.0\n",
      "Loss 92.09087 -- Acc: 0.0\n",
      "Loss 84.05543 -- Acc: 0.0\n",
      "Loss 70.77204 -- Acc: 0.0\n",
      "Loss 79.34556 -- Acc: 0.0\n",
      "Loss 76.24314 -- Acc: 0.0\n",
      "Loss 76.61724 -- Acc: 0.0\n",
      "Loss 77.60274 -- Acc: 0.0\n",
      "Loss 74.11074 -- Acc: 0.0\n",
      "Loss 73.75486 -- Acc: 0.0\n",
      "Loss 87.53244 -- Acc: 0.0\n",
      "Loss 66.51109 -- Acc: 0.0\n",
      "Loss 81.46381 -- Acc: 0.0\n",
      "Avg Loss: 17902.20019 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 3\n",
      "Loss 71.68752 -- Acc: 0.0\n",
      "Loss 64.25822 -- Acc: 0.0\n",
      "Loss 77.9677 -- Acc: 0.0\n",
      "Loss 89.55388 -- Acc: 0.0\n",
      "Loss 67.29787 -- Acc: 0.0\n",
      "Loss 81.32365 -- Acc: 0.0\n",
      "Loss 80.51591 -- Acc: 0.0\n",
      "Loss 83.12415 -- Acc: 0.0\n",
      "Loss 91.43855 -- Acc: 0.0\n",
      "Loss 73.78065 -- Acc: 0.0\n",
      "Loss 71.73784 -- Acc: 0.0\n",
      "Loss 82.63908 -- Acc: 0.0\n",
      "Loss 70.16866 -- Acc: 0.0\n",
      "Loss 75.53464 -- Acc: 0.0\n",
      "Loss 72.00228 -- Acc: 0.0\n",
      "Loss 74.83525 -- Acc: 0.0\n",
      "Loss 77.72122 -- Acc: 0.0\n",
      "Loss 79.69677 -- Acc: 0.0\n",
      "Loss 73.69691 -- Acc: 0.0\n",
      "Loss 68.87765 -- Acc: 0.0\n",
      "Loss 84.66753 -- Acc: 0.0\n",
      "Loss 70.47257 -- Acc: 0.0\n",
      "Loss 91.51109 -- Acc: 0.0\n",
      "Avg Loss: 17911.93253 -- Avg Acc: 0.01559\n",
      "\n",
      "\n",
      "Beginning Epoch 4\n",
      "Loss 89.99382 -- Acc: 0.0\n",
      "Loss 64.64054 -- Acc: 0.0\n",
      "Loss 71.21803 -- Acc: 0.0\n",
      "Loss 82.19976 -- Acc: 0.0\n",
      "Loss 81.47285 -- Acc: 0.0\n",
      "Loss 77.58747 -- Acc: 0.0\n",
      "Loss 74.37291 -- Acc: 0.0\n",
      "Loss 73.88466 -- Acc: 0.0\n",
      "Loss 72.91949 -- Acc: 0.0\n",
      "Loss 77.81999 -- Acc: 0.0\n",
      "Loss 76.7514 -- Acc: 0.0\n",
      "Loss 76.04317 -- Acc: 0.0\n",
      "Loss 78.73096 -- Acc: 0.0\n",
      "Loss 76.70979 -- Acc: 0.0\n",
      "Loss 73.79435 -- Acc: 0.0\n",
      "Loss 87.81569 -- Acc: 0.0\n",
      "Loss 72.05862 -- Acc: 0.0\n",
      "Loss 81.72597 -- Acc: 0.0\n",
      "Loss 77.5417 -- Acc: 0.0\n",
      "Loss 74.96561 -- Acc: 0.0\n",
      "Loss 80.21156 -- Acc: 0.0\n",
      "Loss 77.02017 -- Acc: 0.0\n",
      "Loss 77.86738 -- Acc: 0.0\n",
      "Avg Loss: 17692.45605 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 5\n",
      "Loss 66.29842 -- Acc: 0.0\n",
      "Loss 71.39176 -- Acc: 0.0\n",
      "Loss 79.83213 -- Acc: 0.0\n",
      "Loss 78.78168 -- Acc: 0.0\n",
      "Loss 80.24123 -- Acc: 0.0\n",
      "Loss 71.52975 -- Acc: 0.0\n",
      "Loss 71.6489 -- Acc: 0.0\n",
      "Loss 75.25779 -- Acc: 0.0\n",
      "Loss 73.03783 -- Acc: 0.0\n",
      "Loss 74.92722 -- Acc: 0.0\n",
      "Loss 74.13625 -- Acc: 0.0\n",
      "Loss 78.56201 -- Acc: 0.0\n",
      "Loss 78.29543 -- Acc: 0.0\n",
      "Loss 67.21171 -- Acc: 0.0\n",
      "Loss 77.58284 -- Acc: 0.0\n",
      "Loss 64.79234 -- Acc: 0.0\n",
      "Loss 70.75773 -- Acc: 0.0\n",
      "Loss 65.11437 -- Acc: 0.0\n",
      "Loss 65.26715 -- Acc: 0.0\n",
      "Loss 79.14594 -- Acc: 0.0\n",
      "Loss 74.33862 -- Acc: 0.0\n",
      "Loss 87.91499 -- Acc: 0.0\n",
      "Loss 81.99494 -- Acc: 0.0\n",
      "Avg Loss: 17793.65527 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 6\n",
      "Loss 80.08392 -- Acc: 0.0\n",
      "Loss 79.01215 -- Acc: 0.0\n",
      "Loss 68.07707 -- Acc: 0.0\n",
      "Loss 71.5682 -- Acc: 0.0\n",
      "Loss 95.42755 -- Acc: 0.0\n",
      "Loss 73.76999 -- Acc: 0.0\n",
      "Loss 76.48655 -- Acc: 0.0\n",
      "Loss 69.42309 -- Acc: 0.0\n",
      "Loss 82.90266 -- Acc: 0.0\n",
      "Loss 73.24828 -- Acc: 0.0\n",
      "Loss 78.16748 -- Acc: 0.0\n",
      "Loss 73.18501 -- Acc: 0.0\n",
      "Loss 84.20986 -- Acc: 0.0\n",
      "Loss 67.07353 -- Acc: 0.0\n",
      "Loss 79.58482 -- Acc: 0.0\n",
      "Loss 69.75226 -- Acc: 0.0\n",
      "Loss 74.8223 -- Acc: 0.0\n",
      "Loss 81.47073 -- Acc: 0.0\n",
      "Loss 77.24911 -- Acc: 0.0\n",
      "Loss 82.8879 -- Acc: 0.0\n",
      "Loss 75.82275 -- Acc: 0.0\n",
      "Loss 76.65005 -- Acc: 0.0\n",
      "Loss 74.14484 -- Acc: 0.0\n",
      "Avg Loss: 17807.49895 -- Avg Acc: 0.0\n",
      "\n",
      "\n",
      "Beginning Epoch 7\n",
      "Loss 77.91036 -- Acc: 0.0\n",
      "Loss 75.98425 -- Acc: 0.0\n",
      "Loss 70.21688 -- Acc: 0.0\n",
      "Loss 73.77699 -- Acc: 0.0\n",
      "Loss 70.82665 -- Acc: 0.0\n",
      "Loss 68.76666 -- Acc: 0.0\n",
      "Loss 78.029 -- Acc: 0.0\n",
      "Loss 70.97248 -- Acc: 0.0\n",
      "Loss 69.78043 -- Acc: 0.0\n",
      "Loss 78.81661 -- Acc: 0.0\n",
      "Loss 87.09967 -- Acc: 0.0\n",
      "Loss 72.13876 -- Acc: 0.0\n",
      "Loss 80.33731 -- Acc: 0.0\n",
      "Loss 88.19285 -- Acc: 0.0\n",
      "Loss 68.19586 -- Acc: 0.0\n",
      "Loss 75.47827 -- Acc: 0.0\n",
      "Loss 72.5958 -- Acc: 0.0\n",
      "Loss 71.59941 -- Acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "for epoch in range(n_epochs):\n",
    "    print()\n",
    "    print(\"Beginning Epoch\", epoch)\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for i,data in enumerate(dataloader):\n",
    "        data = {k: v.cuda() for k,v in data.items()}\n",
    "        preds = model(data)\n",
    "        preds = preds[:,:-1].reshape(-1, preds.shape[-1])\n",
    "        labels = data[\"output_ids\"].reshape(-1)\n",
    "        idx = labels!=tokenizer.pad_token_id\n",
    "        loss = loss_fxn(preds[idx], labels[idx])\n",
    "        avg_loss += loss.item()\n",
    "        argmax = torch.argmax(preds[idx])\n",
    "        acc = (argmax==labels[idx]).float().mean()\n",
    "        avg_acc += acc.item()\n",
    "        if i%10==0: print(\"Loss\", round(loss.item(), 5), \"-- Acc:\", round(acc.item(), 5))\n",
    "    print(\"Avg Loss:\", round(avg_loss, 5), \"-- Avg Acc:\", round(avg_acc, 5))\n",
    "    print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb16e4-960e-4db7-a501-31edc66827cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef7cf5-09dc-45e2-bbf7-065b6b086e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eaf26-cfee-4795-98a5-06cd08f96d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
